{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as layers\n",
    "import torch.optim as optimizers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from videoDataLoaderTest import dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import seaborn as sn\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from plot_confusion_matrix import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "from Regular_3D_2D_Hybrid_Extra_Conv2d import Regular_Plus_2dModel\n",
    "from ELU_3D_2D_Hybrid_Extra_Conv2d import ELU_Plus_2dModel\n",
    "from ELU_3D_2D_Hybrid import ELUModel\n",
    "\n",
    "\n",
    "class Hybrid_3D_2D(layers.Module): # layers.Module is what the Hybrid_3D_2d will inherit methods from\n",
    "    # Input layers.Module \n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    # Output\n",
    "    def __init__(self):\n",
    "        # Input self \n",
    "        #\n",
    "        #\n",
    "        # \n",
    "        # Output \n",
    "\n",
    "        super(Hybrid_3D_2D, self).__init__() # \n",
    "        \n",
    "        self.cn1 = layers.Conv3d(in_channels = 30,out_channels = 8,kernel_size = 7, padding = 3) # the images are in  coluor channel and 29 tensors \n",
    "        self.cn2 = layers.Conv3d(in_channels = 8,out_channels = 16,kernel_size = 5, padding = 2)\n",
    "        self.cn3 = layers.Conv3d(in_channels = 16,out_channels=32, kernel_size = 3, padding = 1)\n",
    "        \n",
    "        self.cn1_additional = layers.Conv3d(in_channels = 32,out_channels=64, kernel_size = 3, padding = 1)\n",
    "        self.cn2_additional = layers.Conv3d(in_channels = 64,out_channels=128, kernel_size = 3, padding = 1)\n",
    "        self.cn3_additional = layers.Conv3d(in_channels = 128,out_channels=256, kernel_size = 3, padding = 1)\n",
    "        self.cn4_2d = layers.Conv2d(in_channels = 256, out_channels = 64, kernel_size = 3, padding = 1) # padding for new layers need to be calculated \n",
    "\n",
    "        # size > reduce the input from 3d > 2d layer\n",
    "        # 2d layer output to be flattened \n",
    "        \n",
    "        self.dropout5 = layers.Dropout(0.4)\n",
    "        self.fc5 = layers.Linear(64 * 64 * 64 ,256)\n",
    "        #dropout > 0.4\n",
    "        self.dropout6 = layers.Dropout(0.4)\n",
    "        self.fc6 = layers.Linear(256,128)\n",
    "        self.fc7 = layers.Linear(128,2)\n",
    "        #dropout > 0.4\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        output = self.cn1(x)\n",
    "        output = F.relu(output)\n",
    "\n",
    "        \n",
    "        output = self.cn2(output)\n",
    "        output = F.relu(output)\n",
    "\n",
    "        \n",
    "        output = self.cn3(output)\n",
    "        output = F.relu(output) # activaltion function\n",
    "\n",
    "        output = self.cn1_additional(output)\n",
    "        output = F.relu(output) # activaltion function\n",
    "\n",
    "        output = self.cn2_additional(output)\n",
    "        output = F.relu(output) # activaltion function\n",
    "\n",
    "        output = self.cn3_additional(output)\n",
    "        output = F.relu(output) # activaltion function\n",
    "    \n",
    "        \n",
    "        output = output.reshape(output.size(0), 256,64,64) \n",
    "        \n",
    "        \n",
    "        output = self.cn4_2d(output)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        \n",
    "\n",
    "        output = output.flatten(start_dim=1) # flattens output so that it can be fed into the linear layer\n",
    "\n",
    "        output = self.fc5(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.dropout5(output)\n",
    "\n",
    "\n",
    "        output = self.fc6(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.dropout6(output)\n",
    "        \n",
    "\n",
    "        output = self.fc7(output) \n",
    "\n",
    "        return output\n",
    "\n",
    " \n",
    "y_loss = {}  # loss history\n",
    "y_loss['train'] = []\n",
    "y_loss['val'] = []\n",
    "y_err = {}\n",
    "y_err['train'] = []\n",
    "y_err['val'] = []\n",
    "\n",
    "x_epoch = []\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax0 = fig.add_subplot(121, title=\"Training Loss - Model 2\") \n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "\n",
    "\n",
    "def draw_curve_for_model_2_(current_epoch):\n",
    "    x_epoch.append(current_epoch)\n",
    "    ax0.plot(x_epoch, y_loss['train'], 'bo-', label='Training Loss')\n",
    "    if current_epoch == 0:\n",
    "        ax0.legend()\n",
    "        \n",
    "    fig.savefig(os.path.join('./lossGraphs', 'train_loss_model_2.jpg'))\n",
    "\n",
    "\n",
    "\n",
    "def train(model,device,the_dataloader, optim,epochs,dataset_size,phase,batch_size):\n",
    "    \n",
    "    training_data_predictions = torch.tensor([]) # creates a tensor for storing the predictions of teh model \n",
    "    training_data_actual_values = torch.tensor([])\n",
    "    \n",
    "    \n",
    "    model.train() # setd the model to training mode \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "\n",
    "        running_loss = 0.0 # sets loss for this epoch to 0\n",
    "        running_corrects = 0.0 # sets actual truths to 0 for this epoch\n",
    "\n",
    "        print(\"Current epoch\" + str(epoch))\n",
    "\n",
    "        #start_time = time.time()\n",
    "\n",
    "        loss_list = []    \n",
    "        batch_list = []\n",
    "\n",
    "        predicted_y = []\n",
    "        actual_y = []\n",
    "\n",
    "        for batch_index, (X,y) in enumerate(the_dataloader):\n",
    "            #X,y = X.to(device), y.to(device) # X is the input y is teh ground truth\n",
    "            \n",
    "            optim.zero_grad() # sets the gradients to 0 \n",
    "            prediction = model(X) # gets the predictions made by teh model\n",
    "            \n",
    "            training_data_predictions = torch.cat((training_data_predictions, prediction),dim=0) # combines the predictions of the model for this batch and the ones from before\n",
    "            training_data_actual_values = torch.cat((training_data_actual_values, y),dim=0) # adds the tensors with the actual values\n",
    "\n",
    "            \n",
    "            # parts of code below adapted from \n",
    "            if X.size == 4:\n",
    "                current_batch_size, channels, height, width = X.shape\n",
    "            else:\n",
    "                current_batch_size, framesFed ,channels, height, width = X.shape\n",
    "\n",
    "\n",
    "            if current_batch_size < batch_size: # skips smaller batch to plot the graph\n",
    "                continue\n",
    "\n",
    "\n",
    "            _, preds = torch.max(prediction.data, 1)\n",
    "            loss = layers.CrossEntropyLoss()\n",
    "\n",
    "            \n",
    "            loss_result = loss(prediction,y) # to be chenged to a different one since it does softmax \n",
    "            \n",
    "            \n",
    "            del X\n",
    "\n",
    "            \n",
    "            #loss =  # calculates the loss\n",
    "\n",
    "            loss_result.backward() # updates the weights \n",
    "            optim.step() # \n",
    "            \n",
    "            running_loss+=loss_result.item() * current_batch_size\n",
    "\n",
    "            del loss\n",
    "            running_corrects+=float(torch.sum(preds == y.data))\n",
    "             \n",
    "            \n",
    "            if (batch_index*30) % 30 == 0: # \n",
    "                print(batch_index)\n",
    "                training_result_format = 'batch:({:.0f})|loss:({:.4f}) '.format(batch_index,loss_result)\n",
    "                print(training_result_format)\n",
    "\n",
    "    epoch_loss = running_loss/dataset_size # ;loss fpr this epoch\n",
    "    epoch_acc = running_corrects / dataset_size # \n",
    "    y_loss[phase].append(epoch_loss)\n",
    "    y_err[phase].append(1.0  - epoch_acc)\n",
    "\n",
    "    draw_curve_for_model_2_(epoch)\n",
    "\n",
    "\n",
    "    return model,training_data_predictions,training_data_actual_values\n",
    "\n",
    "def test(model,device,the_dataloader, optim,epochs,dataset_size,phase,batch_size):\n",
    "\n",
    "    testing_data_predictions = torch.tensor([]) # creates a tensor for storing the predictions of teh model \n",
    "    testing_data_actual_values = torch.tensor([])\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_index, (X,y) in enumerate(the_dataloader):\n",
    "        model.eval()\n",
    "        \n",
    "        torch.no_grad()\n",
    "\n",
    "        prediction = model(X) # gets the predictions made by teh model\n",
    "        testing_data_predictions = torch.cat((testing_data_predictions, prediction),dim=0) # combines the predictions of the model for this batch and the ones from before\n",
    "        testing_data_actual_values = torch.cat((testing_data_actual_values, y),dim=0) # adds the tensors with the actual values\n",
    "\n",
    "        # parts of code below adapted from \n",
    "        if X.size == 4:\n",
    "            current_batch_size, channels, height, width = X.shape\n",
    "        else:\n",
    "            current_batch_size, framesFed ,channels, height, width = X.shape\n",
    "        if current_batch_size < batch_size: # skips smaller batch to plot the graph\n",
    "            continue\n",
    "        _, preds = torch.max(prediction.data, 1)\n",
    "        \n",
    "        loss = layers.CrossEntropyLoss()\n",
    "        \n",
    "        loss_result = loss(prediction,y) # to be chenged to a different one since it does softmax \n",
    "        \n",
    "        if (batch_index*30) % 30 == 0: # \n",
    "            print(batch_index)\n",
    "            testing_result_format = 'batch:({:.0f})|loss:({:.4f}) '.format(batch_index,loss_result)\n",
    "            print(testing_result_format)\n",
    "\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        Regular_3D_2D_Hybrid_Time = open(('Model_2_Test_Time/time_file.txt'), \"w\" ) \n",
    "        Regular_3D_2D_Hybrid_Time.write(\"Testing time:\" + str(execution_time) + \"s\" + \"\\n\") \n",
    "        Regular_3D_2D_Hybrid_Time.close()\n",
    "\n",
    "    return model,testing_data_predictions,testing_data_actual_values\n",
    "\n",
    "\n",
    "\n",
    "ReLUModel = Hybrid_3D_2D()\n",
    "\n",
    "optim = optimizers.SGD(ReLUModel.parameters(),lr=0.01)# transfer learning paper > used 0.0005 for transfer learning \n",
    "\n",
    "\n",
    "training_data, testing_data = torch.utils.data.random_split(dataset, [42,18])\n",
    "\n",
    "torch.manual_seed(65) # by setting a seed all random numbers generated can be made the same for all models\n",
    "\n",
    "load_training_data = DataLoader(dataset=training_data, batch_size=6, shuffle=3) \n",
    "load_testing_data = DataLoader(dataset=testing_data, batch_size=6, shuffle=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch0\n",
      "0\n",
      "batch:(0)|loss:(0.6947) \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 24883200 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Aleks\\Desktop\\Results_Code\\Model_2.ipynb Cell 2\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Aleks/Desktop/Results_Code/Model_2.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_results \u001b[39m=\u001b[39m train(ReLUModel, \u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m,load_training_data,optim,\u001b[39m1\u001b[39;49m,\u001b[39mlen\u001b[39;49m(training_data),\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m,batch_size\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Aleks\\Desktop\\Results_Code\\Model_2.ipynb Cell 2\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Aleks/Desktop/Results_Code/Model_2.ipynb#W1sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m predicted_y \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Aleks/Desktop/Results_Code/Model_2.ipynb#W1sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m actual_y \u001b[39m=\u001b[39m []\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Aleks/Desktop/Results_Code/Model_2.ipynb#W1sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_index, (X,y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(the_dataloader):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Aleks/Desktop/Results_Code/Model_2.ipynb#W1sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m     \u001b[39m#X,y = X.to(device), y.to(device) # X is the input y is teh ground truth\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Aleks/Desktop/Results_Code/Model_2.ipynb#W1sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m     optim\u001b[39m.\u001b[39mzero_grad() \u001b[39m# sets the gradients to 0 \u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Aleks/Desktop/Results_Code/Model_2.ipynb#W1sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m     prediction \u001b[39m=\u001b[39m model(X) \u001b[39m# gets the predictions made by teh model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32mc:\\Users\\Aleks\\Desktop\\Results_Code\\video_dataset.py:202\u001b[0m, in \u001b[0;36mVideoFrameDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    198\u001b[0m record: VideoRecord \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvideo_list[idx]\n\u001b[0;32m    200\u001b[0m frame_start_indices: \u001b[39m'\u001b[39m\u001b[39mnp.ndarray[int]\u001b[39m\u001b[39m'\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_start_indices(record)\n\u001b[1;32m--> 202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get(record, frame_start_indices)\n",
      "File \u001b[1;32mc:\\Users\\Aleks\\Desktop\\Results_Code\\video_dataset.py:242\u001b[0m, in \u001b[0;36mVideoFrameDataset._get\u001b[1;34m(self, record, frame_start_indices)\u001b[0m\n\u001b[0;32m    239\u001b[0m             frame_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    241\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 242\u001b[0m     images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(images)\n\u001b[0;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m images, record\u001b[39m.\u001b[39mlabel\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Aleks\\Desktop\\Results_Code\\video_dataset.py:267\u001b[0m, in \u001b[0;36mImglistToTensor.forward\u001b[1;34m(img_list)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    256\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(img_list: List[Image\u001b[39m.\u001b[39mImage]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtorch.Tensor[NUM_IMAGES, CHANNELS, HEIGHT, WIDTH]\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    257\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39m    Converts each PIL image in a list to\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[39m    a torch Tensor and stacks them into\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39m        tensor of size ``NUM_IMAGES x CHANNELS x HEIGHT x WIDTH``\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([transforms\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mto_tensor(pic) \u001b[39mfor\u001b[39;00m pic \u001b[39min\u001b[39;00m img_list])\n",
      "File \u001b[1;32mc:\\Users\\Aleks\\Desktop\\Results_Code\\video_dataset.py:267\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    256\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(img_list: List[Image\u001b[39m.\u001b[39mImage]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtorch.Tensor[NUM_IMAGES, CHANNELS, HEIGHT, WIDTH]\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    257\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39m    Converts each PIL image in a list to\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[39m    a torch Tensor and stacks them into\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39m        tensor of size ``NUM_IMAGES x CHANNELS x HEIGHT x WIDTH``\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([transforms\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mto_tensor(pic) \u001b[39mfor\u001b[39;00m pic \u001b[39min\u001b[39;00m img_list])\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:171\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    169\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[1;32m--> 171\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49mdefault_float_dtype)\u001b[39m.\u001b[39;49mdiv(\u001b[39m255\u001b[39;49m)\n\u001b[0;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 24883200 bytes."
     ]
    }
   ],
   "source": [
    "training_results = train(ReLUModel, 'cpu',load_training_data,optim,150,len(training_data),'train',batch_size=6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_trained_3D_2D_Hybrid = training_results[0]\n",
    "\n",
    "\n",
    "test_results = test(the_trained_3D_2D_Hybrid, 'cpu',load_testing_data,optim,150,len(testing_data),'val',batch_size=6) \n",
    "\n",
    "training_predictions = (training_results[1]).detach().argmax(dim=1)\n",
    "training_data_actual_values = (training_results[2])\n",
    "\n",
    "classes = [\"No Violence\",\"Violence\"]\n",
    "\n",
    "confusion_mat = confusion_matrix(training_predictions,training_data_actual_values)\n",
    "plt.figure(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Confusion Matrix Plotting Section\n",
    "plot_confusion_matrix(confusion_mat,classes,title='Confusion Matrix ')\n",
    "\n",
    "print(type(confusion_mat))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "## end of Plotting Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_predictions = (test_results[1]).detach().argmax(dim=1)\n",
    "testing_data_actual_values = (test_results[2])\n",
    "\n",
    "classes = [\"No Violence\",\"Violence\"]\n",
    "\n",
    "confusion_mat = confusion_matrix(testing_predictions,testing_data_actual_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Confusion Matrix Plotting Section\n",
    "plot_confusion_matrix(confusion_mat,classes,title='Confusion Matrix ')\n",
    "\n",
    "print(type(confusion_mat))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.savefig(os.path.join('./Confusion Matrices', 'reg_model_confuse_matrix2.jpg'))\n",
    "\n",
    "## end of Plotting Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
